# -*- coding: utf-8 -*-
"""Aditi_Yadav_Taxi Fare Prediction System_MLOps.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14IbAkru8w_KKoThXc_-dKUXWycXhzBAX

## Objective:

The primary objective of this project is to develop a predictive model that can accurately estimate the total fare of a taxi trip given various input features such as pickup and drop-off locations, trip distance, time of day, and trafÔ¨Åc conditions. The challenges in this problem include dealing with large
datasets, extracting meaningful features etc.
"""

!pip install dask[complete] s3fs

!pip install "fsspec==2023.6.0"  # Reverting to a version compatible with gcsfs

import os

# Set AWS credentials
os.environ['AWS_ACCESS_KEY_ID'] = <access_key>
os.environ['AWS_SECRET_ACCESS_KEY'] = <secret_key>
os.environ['AWS_REGION'] = 'us-east-1'

import dask.dataframe as dd
from dask.distributed import Client

client = Client()  # Starts a local Dask client

!pip install pandas numpy matplotlib seaborn scikit-learn

# Commented out IPython magic to ensure Python compatibility.
#Importing required libraries
import os #getting access to input files
import pandas as pd # Importing pandas for performing EDA
import numpy as np  # Importing numpy for Linear Algebric operations
import matplotlib.pyplot as plt # Importing for Data Visualization
import seaborn as sns # Importing for Data Visualization
from collections import Counter
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import LinearRegression #ML algorithm
from sklearn.model_selection import train_test_split #splitting dataset
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from pprint import pprint
from sklearn.model_selection import GridSearchCV

# %matplotlib inline

train_path = 's3://clean-taxi-data/taxi_trained_data.csv'

"""## Distributed Processing with Dask"""

train = dd.read_csv(train_path, storage_options={'key': os.environ['AWS_ACCESS_KEY_ID'], 'secret': os.environ['AWS_SECRET_ACCESS_KEY']})

"""## Understanding the data :"""

train.head() #checking first five rows of the training dataset

print("shape of training data is: ",train.shape) #checking the number of rows and columns in training data

train.dtypes #checking the data-types in training dataset

"""Here we can see pickup datetime is of string and fare amount is of float type. So we need to change the data type of both."""

train.describe()

"""## Data Cleaning & Missing Value Analysis :"""

train_pd = train.compute()  # Convert to Pandas DataFrame for detailed manipulation

# Now convert 'fare_amount' to numeric, coercing errors to NaN
train_pd['fare_amount'] = pd.to_numeric(train_pd['fare_amount'], errors='coerce')

train_pd.dtypes

train_pd.shape

train_pd.dropna(subset= ["pickup_datetime"])   #dropping NA values in datetime column

# Here pickup_datetime variable is in object so we need to change its data type to datetime
train_pd['pickup_datetime'] =  pd.to_datetime(train_pd['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC')

### we will saperate the Pickup_datetime column into separate field like year, month, day of the week, etc

train_pd['year'] = train_pd['pickup_datetime'].dt.year
train_pd['Month'] = train_pd['pickup_datetime'].dt.month
train_pd['Date'] = train_pd['pickup_datetime'].dt.day
train_pd['Day'] = train_pd['pickup_datetime'].dt.dayofweek
train_pd['Hour'] = train_pd['pickup_datetime'].dt.hour
train_pd['Minute'] = train_pd['pickup_datetime'].dt.minute

train_pd.dtypes #Re-checking datatypes after conversion

"""#### Observations :

1. Passenger count should not exceed 6(even if we consider SUV)
2. Latitudes range from -90 to 90. Longitudes range from -180 to 180
3. Few missing values and High values of fare and Passenger count are present. So, decided to remove them.

Checking the Datetime Variable :
"""

#removing datetime missing values rows
train_pd = train_pd.drop(train_pd[train_pd['pickup_datetime'].isnull()].index, axis=0)
print(train_pd.shape)
print(train_pd['pickup_datetime'].isnull().sum())

"""Checking the passenger count variable :"""

train_pd["passenger_count"].describe()

"""We can see maximum number of passanger count is 208 which is actually not possible. So reducing the passenger count to 6 (even if we consider the SUV)"""

train_pd = train_pd.drop(train_pd[train_pd["passenger_count"]> 6 ].index, axis=0)

#Also removing the values with passenger count of 0.
train_pd = train_pd.drop(train_pd[train_pd["passenger_count"] == 0 ].index, axis=0)

train_pd["passenger_count"].describe()

train_pd["passenger_count"].sort_values(ascending= True)

#removing passanger_count missing values rows
train_pd = train_pd.drop(train_pd[train_pd['passenger_count'].isnull()].index, axis=0)
print(train_pd.shape)
print(train_pd['passenger_count'].isnull().sum())

"""There is one passenger count value of 0.12 which is not possible. Hence we will remove fractional passenger value"""

train_pd = train_pd.drop(train_pd[train_pd["passenger_count"] == 0.12 ].index, axis=0)
train_pd.shape

"""Next checking the Fare Amount variable :"""

##finding decending order of fare to get to know whether the outliers are present or not
train_pd["fare_amount"].sort_values(ascending=False)

Counter(train_pd["fare_amount"]<0)

train_pd = train_pd.drop(train_pd[train_pd["fare_amount"]<0].index, axis=0)
train_pd.shape

##make sure there is no negative values in the fare_amount variable column
train_pd["fare_amount"].min()

#Also remove the row where fare amount is zero
train_pd = train_pd.drop(train_pd[train_pd["fare_amount"]<1].index, axis=0)
train_pd.shape

#Now we can see that there is a huge difference in 1st 2nd and 3rd position in decending order of fare amount
# so we will remove the rows having fare amounting more that 454 as considering them as outliers

train_pd = train_pd.drop(train_pd[train_pd["fare_amount"]> 454 ].index, axis=0)
train_pd.shape

# eliminating rows for which value of "fare_amount" is missing
train_pd = train_pd.drop(train_pd[train_pd['fare_amount'].isnull()].index, axis=0)
print(train_pd.shape)
print(train_pd['fare_amount'].isnull().sum())

train_pd["fare_amount"].describe()

"""Now checking the pickup lattitude and longitude :"""

#Lattitude----(-90 to 90)
#Longitude----(-180 to 180)

# we need to drop the rows having  pickup lattitute and longitute out the range mentioned above

#train = train.drop(train[train['pickup_latitude']<-90])
train_pd[train_pd['pickup_latitude']<-90]
train_pd[train_pd['pickup_latitude']>90]

#Hence dropping one value of >90
train_pd = train_pd.drop((train_pd[train_pd['pickup_latitude']<-90]).index, axis=0)
train_pd = train_pd.drop((train_pd[train_pd['pickup_latitude']>90]).index, axis=0)

train_pd[train_pd['pickup_longitude']<-180]
train_pd[train_pd['pickup_longitude']>180]

train_pd[train_pd['dropoff_latitude']<-90]
train_pd[train_pd['dropoff_latitude']>90]

train_pd[train_pd['dropoff_longitude']<-180]
train_pd[train_pd['dropoff_longitude']>180]

train_pd.shape

train_pd.isnull().sum()

"""### Now we have successfully cleaned our training dataset. Thus proceeding for further operations:

Calculating distance based on the given coordinates :
"""

#As we know that we have given pickup longitute and latitude values and same for drop.
#So we need to calculate the distance Using the haversine formula and we will create a new variable called distance
from math import radians, cos, sin, asin, sqrt

def haversine(a):
    lon1=a[0]
    lat1=a[1]
    lon2=a[2]
    lat2=a[3]
    """
    Calculate the great circle distance between two points
    on the earth (specified in decimal degrees)
    """
    # convert decimal degrees to radians
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])

    # haversine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c =  2 * asin(sqrt(a))
    # Radius of earth in kilometers is 6371
    km = 6371* c
    return km
# 1min

train_pd['distance'] = train_pd[['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']].apply(haversine,axis=1)

train_pd.head()

train_pd.nunique()

##finding decending order of fare to get to know whether the outliers are presented or not
train_pd['distance'].sort_values(ascending=False)

"""The sorted list of distances from highest to lowest reveals potential outliers in the dataset. The top values, such as 19,688.8 km, far exceed plausible taxi ride distances within a typical city or even across most countries. This suggests errors or anomalies in the data. For instance, a distance value of 19,688.8 km could result from incorrect latitude and longitude coordinates or data entry errors.


"""

Counter(train_pd['distance'] == 0)

Counter(train_pd['fare_amount'] == 0)

train_pd = train_pd[train_pd['distance'] != 0]

train_pd.shape

#we will remove the rows whose distance values is very high which is more than 129kms
train_pd = train_pd.drop(train_pd[train_pd['distance'] > 130 ].index, axis=0)
train_pd.shape

train_pd.head()

"""Now we have splitted the pickup date time variable into different varaibles like month, year, day etc so now we dont need to have that pickup_Date variable now. Hence we can drop that, Also we have created distance using pickup and drop longitudes and latitudes so we will also drop pickup and drop longitudes and latitudes variables."""

drop = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude','dropoff_longitude', 'dropoff_latitude', 'Minute']
train_pd = train_pd.drop(drop, axis = 1)

train_pd.head()

train_pd['passenger_count'] = train_pd['passenger_count'].astype('int64')
train_pd['year'] = train_pd['year'].astype('int64')
train_pd['Month'] = train_pd['Month'].astype('int64')
train_pd['Date'] = train_pd['Date'].astype('int64')
train_pd['Day'] = train_pd['Day'].astype('int64')
train_pd['Hour'] = train_pd['Hour'].astype('int64')

train_pd.dtypes

"""# Data Visualization :

Visualization of following:

1. Number of Passengers effects the fare
2. Pickup date and time effects the fare
3. Day of the week does effects the fare
4. Distance effects the fare
"""

# Count plot on passenger count
plt.figure(figsize=(15,7))
sns.countplot(x="passenger_count", data=train_pd)

#Relationship beetween number of passengers and Fare

plt.figure(figsize=(15,7))
plt.scatter(x=train_pd['passenger_count'], y=train_pd['fare_amount'], s=10)
plt.xlabel('No. of Passengers')
plt.ylabel('Fare')
plt.show()

"""### Observations :
   By seeing the above plots we can easily conclude that:
1. single travelling passengers are most frequent travellers.
2. At the sametime we can also conclude that highest Fare are coming from single & double travelling passengers.
"""

#Relationship between date and Fare
plt.figure(figsize=(15,7))
plt.scatter(x=train_pd['Date'], y=train_pd['fare_amount'], s=10)
plt.xlabel('Date')
plt.ylabel('Fare')
plt.show()

plt.figure(figsize=(15,7))
train_pd.groupby(train_pd["Hour"])['Hour'].count().plot(kind="bar")
plt.show()

"""Lowest cabs at 5 AM and highest at and around 7 PM i.e the office rush hours"""

#Relationship between Time and Fare
plt.figure(figsize=(15,7))
plt.scatter(x=train_pd['Hour'], y=train_pd['fare_amount'], s=10)
plt.xlabel('Hour')
plt.ylabel('Fare')
plt.show()

"""From the above plot We can observe that the cabs taken at 7 am and 23 Pm are the costliest.
Hence we can assume that cabs taken early in morning and late at night are costliest

"""

#impact of Day on the number of cab rides
plt.figure(figsize=(15,7))
sns.countplot(x="Day", data=train_pd)

"""Observation :
The day of the week does not seem to have much influence on the number of cabs ride
"""

#Relationships between day and Fare
plt.figure(figsize=(15,7))
plt.scatter(x=train_pd['Day'], y=train_pd['fare_amount'], s=10)
plt.xlabel('Day')
plt.ylabel('Fare')
plt.show()

"""The highest fares seem to be on a Sunday, and Thursday, and the low on Wednesday and Monday. May be due to low demand of the cabs on saturdays the cab fare is low and high demand of cabs on sunday and monday shows the high fare prices"""

#Relationship between distance and fare
plt.figure(figsize=(15,7))
plt.scatter(x = train_pd['distance'],y = train_pd['fare_amount'],c = "g")
plt.xlabel('Distance')
plt.ylabel('Fare')
plt.show()

"""It is quite obvious that distance will effect the amount of fare

# Feature Scaling :
"""

#Normality check of training data is uniformly distributed or not-

# for i in ['fare_amount', 'distance']:
#     print(i)
#     sns.distplot(train_pd[i],bins='auto',color='green')
#     plt.title("Distribution for Variable "+i)
#     plt.ylabel("Density")
#     plt.show()



import seaborn as sns
import matplotlib.pyplot as plt

# Normality check of training data is uniformly distributed or not
for i in ['fare_amount', 'distance']:
    print(i)
    sns.histplot(train_pd[i], kde=True, color='green', stat="density", bins='auto')
    plt.title("Distribution for Variable "+i)
    plt.ylabel("Density")
    plt.show()

#since skewness of target variable is high, apply log transform to reduce the skewness-
train_pd['fare_amount'] = np.log1p(train_pd['fare_amount'])

#since skewness of distance variable is high, apply log transform to reduce the skewness-
train_pd['distance'] = np.log1p(train_pd['distance'])

#Normality Re-check to check data is uniformly distributed or not after log transformartion
import seaborn as sns
import matplotlib.pyplot as plt

for i in ['fare_amount', 'distance']:
    print(i)
    sns.histplot(train_pd[i], bins='auto', color='green', kde=True, stat="density")
    plt.title("Distribution for Variable " + i)
    plt.ylabel("Density")
    plt.show()

"""Here we can see bell shaped distribution. Hence our continous variables are now normally distributed, we will use not use any  Feature Scalling technique. i.e, Normalization or Standarization for our training data

As we can see a bell shaped distribution. Hence our continous variables are now normally distributed, we will use not use any  Feature Scalling technique. i.e, Normalization or Standarization for our test data

# Applying ML ALgorithms:
"""

##train test split for further modelling
X_train, X_test, y_train, y_test = train_test_split( train_pd.iloc[:, train_pd.columns != 'fare_amount'],
                         train_pd.iloc[:, 0], test_size = 0.20, random_state = 1)

print(X_train.shape)
print(X_test.shape)

"""### Linear Regression Model :"""

# Building model on top of training dataset
X_train = train_pd.drop(['key', 'fare_amount'], axis=1)  # Drop 'key' if it's not needed
y_train = train_pd['fare_amount'].astype(float)  # Ensure y_train is numeric


fit_LR = LinearRegression().fit(X_train , y_train)

#prediction on train data
pred_train_LR = fit_LR.predict(X_train)

print("Training columns:", X_train.columns)
print("Testing columns:", X_test.columns)

# Drop the 'key' column from X_test
X_test = X_test.drop('key', axis=1)

# Now you can make predictions with the trained model
pred_test_LR = fit_LR.predict(X_test)

##calculating RMSE for test data


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# Prepare the feature matrix X by dropping the 'key' and 'fare_amount' columns
X = train_pd.drop(['key', 'fare_amount'], axis=1)  # We're excluding 'key' because it's likely an identifier

# Prepare the target vector y
y = train_pd['fare_amount'].astype(float)  # Converting to float to ensure numerical calculations

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build the Linear Regression model
model = LinearRegression().fit(X_train, y_train)

# Predict on training and testing data
pred_train_LR = model.predict(X_train)
pred_test_LR = model.predict(X_test)

# Calculate RMSE for training and testing datasets
RMSE_train_LR = np.sqrt(mean_squared_error(y_train, pred_train_LR))
RMSE_test_LR = np.sqrt(mean_squared_error(y_test, pred_test_LR))

# Print the RMSE values
print("RMSE on Training Data:", RMSE_train_LR)
print("RMSE on Testing Data:", RMSE_test_LR)

#calculate R^2 for train data
from sklearn.metrics import r2_score
r2_score(y_train, pred_train_LR)

r2_score(y_test, pred_test_LR)

"""### Decision tree Model :"""

fit_DT = DecisionTreeRegressor(max_depth = 2).fit(X_train,y_train)

#prediction on train data
pred_train_DT = fit_DT.predict(X_train)

#prediction on test data
pred_test_DT = fit_DT.predict(X_test)

##calculating RMSE for train data
RMSE_train_DT = np.sqrt(mean_squared_error(y_train, pred_train_DT))

##calculating RMSE for test data
RMSE_test_DT = np.sqrt(mean_squared_error(y_test, pred_test_DT))

print("Root Mean Squared Error For Training data = "+str(RMSE_train_DT))
print("Root Mean Squared Error For Test data = "+str(RMSE_test_DT))

## R^2 calculation for train data
r2_score(y_train, pred_train_DT)

## R^2 calculation for test data
r2_score(y_test, pred_test_DT)

"""### Random Forest Model :"""

fit_RF = RandomForestRegressor(n_estimators=30,max_depth=10,n_jobs=-1,random_state=42).fit(X_train,y_train)

#prediction on train data
pred_train_RF = fit_RF.predict(X_train)
#prediction on test data
pred_test_RF = fit_RF.predict(X_test)

##calculating RMSE for train data
RMSE_train_RF = np.sqrt(mean_squared_error(y_train, pred_train_RF))
##calculating RMSE for test data
RMSE_test_RF = np.sqrt(mean_squared_error(y_test, pred_test_RF))

print("Root Mean Squared Error For Training data = "+str(RMSE_train_RF))
print("Root Mean Squared Error For Test data = "+str(RMSE_test_RF))

## calculate R^2 for train data

r2_score(y_train, pred_train_RF)

#calculate R^2 for test data
r2_score(y_test, pred_test_RF)

"""### Gradient Boosting :"""

fit_GB = GradientBoostingRegressor(n_estimators=20,max_depth=5,learning_rate=0.1,random_state=42).fit(X_train, y_train)

#prediction on train data
pred_train_GB = fit_GB.predict(X_train)

#prediction on test data
pred_test_GB = fit_GB.predict(X_test)

##calculating RMSE for train data
RMSE_train_GB = np.sqrt(mean_squared_error(y_train, pred_train_GB))
##calculating RMSE for test data
RMSE_test_GB = np.sqrt(mean_squared_error(y_test, pred_test_GB))

print("Root Mean Squared Error For Training data = "+str(RMSE_train_GB))
print("Root Mean Squared Error For Test data = "+str(RMSE_test_GB))

#calculate R^2 for test data
r2_score(y_test, pred_test_GB)

#calculate R^2 for train data
r2_score(y_train, pred_train_GB)

"""# Optimizing the results with parameters tuning :"""

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state = 42)
from pprint import pprint
# Look at parameters used by our current forest
print('Parameters currently in use:\n')
pprint(rf.get_params())

##Random Hyperparameter Grid

from sklearn.model_selection import train_test_split,RandomizedSearchCV



from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the RandomForestRegressor with a random state for reproducibility
RRF = RandomForestRegressor(random_state=0)

# Define a smaller range for n_estimators and less variation in max_depth
n_estimator = list(range(10, 51, 10))  # Reduced maximum number of estimators
depth = list(range(3, 15, 3))  # Smaller range of depths for quicker evaluations

# Create the random grid
rand_grid = {
    'n_estimators': n_estimator,
    'max_depth': depth
}

# Initialize RandomizedSearchCV with fewer iterations and CV folds for speed
randomcv_rf = RandomizedSearchCV(
    RRF,
    param_distributions=rand_grid,
    n_iter=3,  # Reduced number of iterations
    cv=3,  # Fewer CV folds
    random_state=0,
    n_jobs=-1  # Use all available cores
)

# Fit the model on the training data
randomcv_rf.fit(X_train, y_train)

# Predict on the test set
predictions_RRF = randomcv_rf.predict(X_test)

# Retrieve the best parameters and the best model
view_best_params_RRF = randomcv_rf.best_params_
best_model = randomcv_rf.best_estimator_

# Evaluate the model
RRF_r2 = r2_score(y_test, predictions_RRF)
RRF_rmse = np.sqrt(mean_squared_error(y_test, predictions_RRF))

print('Random Search CV Random Forest Regressor Model Performance:')
print('Best Parameters = ', view_best_params_RRF)
print('R-squared = {:.2f}.'.format(RRF_r2))
print('RMSE = {:.2f}'.format(RRF_rmse))

gb = GradientBoostingRegressor(random_state = 42)
from pprint import pprint
# Look at parameters used by our current forest
print('Parameters currently in use:\n')
pprint(gb.get_params())

"""# Prediction of fare from provided test dataset :

We have already cleaned and processed our test dataset along with our training dataset. Hence we will be predicting using grid search CV for random forest model
"""

best_model = randomcv_rf.best_estimator_

test_path = 's3://taxi-data-raw/taxi_test_data.csv'

test_pd = pd.read_csv(test_path)

test_pd.head()

test_pd['pickup_datetime'] = pd.to_datetime(test_pd['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC')

# Extract features like hour or other components if used in training
test_pd['hour'] = test_pd['pickup_datetime'].dt.hour
test_pd['day_of_week'] = test_pd['pickup_datetime'].dt.dayofweek



test_features = test_pd.drop(['key', 'pickup_datetime'], axis=1)

from math import radians, cos, sin, asin, sqrt
import pandas as pd

# Haversine function definition that accepts four separate parameters
def haversine(lon1, lat1, lon2, lat2):
    """
    Calculate the great circle distance between two points
    on the earth (specified in decimal degrees)
    """
    # convert decimal degrees to radians
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])

    # haversine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    # Radius of earth in kilometers is 6371
    km = 6371 * c
    return km


# Calculate distance for each row in test_pd using the corrected haversine function
test_pd['distance'] = test_pd.apply(lambda row: haversine(row['pickup_longitude'], row['pickup_latitude'], row['dropoff_longitude'], row['dropoff_latitude']), axis=1)

# Extract additional features from pickup_datetime
test_pd['pickup_datetime'] = pd.to_datetime(test_pd['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC')
test_pd['year'] = test_pd['pickup_datetime'].dt.year
test_pd['Month'] = test_pd['pickup_datetime'].dt.month
test_pd['Date'] = test_pd['pickup_datetime'].dt.day
test_pd['Day'] = test_pd['pickup_datetime'].dt.dayofweek
test_pd['Hour'] = test_pd['pickup_datetime'].dt.hour

# Select the appropriate features that were used in the model
test_features = test_pd[['passenger_count', 'year', 'Month', 'Date', 'Day', 'Hour', 'distance']]

# Predict using the best model
predictions = best_model.predict(test_features)
test_pd['predicted_fare_amount'] = predictions

# Display the first few rows to verify predictions
print(test_pd[['key', 'predicted_fare_amount']].head())

test_pd.head()

test_pd.to_csv('predicted_fare_amounts.csv', index=False)

bucket_name = 'clean-taxi-data'

# Specify the object name in S3
object_name = 'predicted_fare_amounts.csv'


train_pd.to_csv('predicted_fare_amounts.csv', index=False)

!pip install boto3

import boto3

# Create an S3 client
s3_client = boto3.client(
    's3',
    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],
    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],
    region_name=os.environ['AWS_REGION']
)

# Specify your bucket name
bucket_name = 'clean-taxi-data'

# Specify the object name in S3
object_name = 'predicted_fare_amounts.csv'

# Upload the file
s3_client.upload_file('predicted_fare_amounts.csv', bucket_name, object_name)

